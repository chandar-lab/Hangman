game: hangman

# Parallel per-GPU providers (4× A100)
providers:
  main_pool: [qwen3_14b_vllm_hermes_gpu0, qwen3_14b_vllm_hermes_gpu1, qwen3_14b_vllm_hermes_gpu2, qwen3_14b_vllm_hermes_gpu3]
  player_pool: [qwen3_14b_vllm_hermes_gpu0, qwen3_14b_vllm_hermes_gpu1, qwen3_14b_vllm_hermes_gpu2, qwen3_14b_vllm_hermes_gpu3]
  judge_pool: [kimi_k2_openrouter]
  # Fallbacks (unused when pools are present)
  main: qwen3_14b_vllm_hermes_gpu0
  player: qwen3_14b_vllm_hermes_gpu0
  judge: kimi_k2_openrouter

# Agents (8): Vanilla, PrivateCoT, ReActMem×3, Workflow×3
agents:
  - VanillaLLMAgent:
      name: vanilla_llm_agent
  - PrivateCoTAgent:
      name: private_cot_agent
  - ReActMemAgent:
      strategy: overwrite
      name: reactmem_overwrite_agent
  - ReActMemAgent:
      strategy: patch_and_replace
      name: reactmem_patch_and_replace_agent
  - ReActMemAgent:
      strategy: append_and_delete
      name: reactmem_append_and_delete_agent
  - WorkflowAgent:
      strategy: overwrite
      name: workflow_overwrite_agent
  - WorkflowAgent:
      strategy: patch_and_replace
      name: workflow_patch_and_replace_agent
  - WorkflowAgent:
      strategy: append_and_delete
      name: workflow_append_and_delete_agent

# Trials and turns
num_trials: 50
max_turns: 25

# Results root for Qwen3 experiments
results_dir: results/qwen_3/hangman/

first_mover: player

evaluator:
  mode: both
  judge_llm_provider: kimi_k2_openrouter


