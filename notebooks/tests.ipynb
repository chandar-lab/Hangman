{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654ec1eb",
   "metadata": {},
   "source": [
    "### Test LLM Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f43839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# --- Setup Project Path ---\n",
    "# This allows the notebook to find your 'src' directory and the 'hangman' package\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "from src.hangman.providers.llmprovider import load_llm_provider, ModelOutput\n",
    "\n",
    "# --- Test Configuration ---\n",
    "CONFIG_PATH = \"../config.yaml\"\n",
    "QWEN_PROVIDER_NAME = \"qwen3_14b_local\"\n",
    "KIMI_PROVIDER_NAME = \"kimi_k2_openrouter\"\n",
    "\n",
    "# --- Reusable Test Function ---\n",
    "def test_provider(provider_name: str):\n",
    "    \"\"\"Loads a provider by name, invokes it with a test prompt, and prints the results.\"\"\"\n",
    "    print(f\"Loading provider '{provider_name}'...\")\n",
    "    try:\n",
    "        llm_provider = load_llm_provider(config_path=CONFIG_PATH, provider_name=provider_name)\n",
    "        print(\"‚úÖ LLM Provider loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load LLM Provider: {e}\")\n",
    "        return\n",
    "\n",
    "    # Create a sample conversation to send to the model\n",
    "    sample_messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant. Please use <think> tags to outline your reasoning before providing the final answer.\"),\n",
    "        HumanMessage(content=\"What is the capital of France? Explain your reasoning process step-by-step.\")\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nInvoking model with a test prompt (thinking enabled)...\")\n",
    "    # Invoke the model, requesting the thinking trace\n",
    "    output = llm_provider.invoke(sample_messages, thinking=True)\n",
    "\n",
    "    # Print the structured output\n",
    "    print(\"\\n--- ü§î Thinking Process ---\")\n",
    "    print(output.get(\"thinking\") or \"No thinking trace found (as expected for this model).\")\n",
    "\n",
    "    print(\"\\n--- üí¨ Final Response ---\")\n",
    "    print(output.get(\"response\") or \"No response found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a931a822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üß™ TESTING QWEN PROVIDER ---\n",
      "Loading provider 'qwen3_14b_local'...\n",
      "‚úÖ LLM Provider loaded successfully.\n",
      "\n",
      "Invoking model with a test prompt (thinking enabled)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 13:35:29,550 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 13:35:29,551 - WARNING - No thinking tags found in the response. Treating as direct response. The response is:\n",
      "--\n",
      "\n",
      "\n",
      "The capital of France is **Paris**. Here's the step-by-step reasoning:\n",
      "\n",
      "1. **Geographical Knowledge**: France is a country in Western Europe. Its capital is widely recognized as Paris, a major global city known for landmarks like the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\n",
      "\n",
      "2. **Political Center**: Paris serves as the political, economic, and cultural hub of France. The French government, including the President's residence (√âlys√©e Palace) and the National Assembly, is based there.\n",
      "\n",
      "3. **Historical Context**: Paris has been the capital of France since the 13th century, even through periods of political upheaval (e.g., the French Revolution, World War II). This historical continuity reinforces its role as the capital.\n",
      "\n",
      "4. **Elimination of Alternatives**: Other major French cities like Lyon, Marseille, or Bordeaux are not capitals. They are renowned for other reasons (e.g., Lyon for gastronomy, Marseille for its port) but lack the political and administrative significance of Paris.\n",
      "\n",
      "5. **Global Recognition**: Paris is universally acknowledged as France's capital in international contexts, confirmed by travel guides, maps, and diplomatic references.\n",
      "\n",
      "Thus, the conclusion is clear: **Paris is the capital of France**.\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ü§î Thinking Process ---\n",
      "No thinking trace found (as expected for this model).\n",
      "\n",
      "--- üí¨ Final Response ---\n",
      "\n",
      "\n",
      "The capital of France is **Paris**. Here's the step-by-step reasoning:\n",
      "\n",
      "1. **Geographical Knowledge**: France is a country in Western Europe. Its capital is widely recognized as Paris, a major global city known for landmarks like the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\n",
      "\n",
      "2. **Political Center**: Paris serves as the political, economic, and cultural hub of France. The French government, including the President's residence (√âlys√©e Palace) and the National Assembly, is based there.\n",
      "\n",
      "3. **Historical Context**: Paris has been the capital of France since the 13th century, even through periods of political upheaval (e.g., the French Revolution, World War II). This historical continuity reinforces its role as the capital.\n",
      "\n",
      "4. **Elimination of Alternatives**: Other major French cities like Lyon, Marseille, or Bordeaux are not capitals. They are renowned for other reasons (e.g., Lyon for gastronomy, Marseille for its port) but lack the political and administrative significance of Paris.\n",
      "\n",
      "5. **Global Recognition**: Paris is universally acknowledged as France's capital in international contexts, confirmed by travel guides, maps, and diplomatic references.\n",
      "\n",
      "Thus, the conclusion is clear: **Paris is the capital of France**.\n"
     ]
    }
   ],
   "source": [
    "# --- Run Tests ---\n",
    "print(\"--- üß™ TESTING QWEN PROVIDER ---\")\n",
    "test_provider(QWEN_PROVIDER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96016d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_provider = load_llm_provider(config_path=CONFIG_PATH, provider_name=QWEN_PROVIDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18f2981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 13:49:39,444 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = llm_provider.client.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Ti vedo bene o no?\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11518b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user asked, \"Ti vedo bene o no?\" which translates to \"Can you see me well or not?\" in Italian. I need to figure out the best way to respond.\\n\\nFirst, I should acknowledge that as an AI, I don\\'t have a visual component. I can\\'t see images or videos. But maybe the user is asking if I can understand them properly. They might be concerned about whether I can process their input correctly, especially if they sent an image or video that I can\\'t see.\\n\\nI should explain that I can\\'t see visual content but can help with text. Also, offer assistance with any text-based questions or tasks. Maybe they want to know if I can interpret their message accurately. I need to be clear but helpful, ensuring they know my limitations and how I can still assist them.\\n</think>\\n\\nNon ho la capacit√† di \"vedere\" immagini, video o oggetti fisici, poich√© sono un modello di intelligenza artificiale basato su testo. Tuttavia, posso aiutarti a rispondere a domande, analizzare testi, creare contenuti o risolvere problemi in modo logico e razionale. Se hai qualcosa da condividere o un problema da risolvere, fammi sapere e far√≤ del mio meglio per aiutarti! üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 288, 'prompt_tokens': 15, 'total_tokens': 303, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen3-14B', 'system_fingerprint': None, 'id': 'chatcmpl-f5e06dc6daa147f5ab63ba2e46f745d8', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--a000e071-b181-453d-bdae-71ced699bf33-0', usage_metadata={'input_tokens': 15, 'output_tokens': 288, 'total_tokens': 303, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5580f",
   "metadata": {},
   "source": [
    "AIMessage(content=\"\\n\\nThe capital of France is **Paris**. Here's the reasoning process step-by-step:\\n\\n1. **Definition of a Capital**: A capital city is the seat of a country's government, housing key institutions like the executive branch (e.g., the president's residence), legislative bodies (e.g., parliament), and administrative centers.\\n\\n2. **Government Institutions in Paris**:\\n   - The **√âlys√©e Palace** in Paris is the official residence of the President of France (currently Emmanuel Macron).\\n   - The **French Parliament** (National Assembly and Senate) meets in Paris, with the National Assembly located in the **Palais Bourbon** and the Senate in the **Palais du Luxembourg**.\\n\\n3. **Historical Context**: Paris has been the political and cultural heart of France for centuries. It became the de facto capital during the Middle Ages and remained so through the French Revolution and modern times.\\n\\n4. **Geographical and Cultural Recognition**: Paris is widely recognized globally as France's capital, known for landmarks like the Eiffel Tower, Louvre Museum, and its role in art, fashion, and politics.\\n\\n5. **Elimination of Alternatives**: Other major French cities (e.g., Lyon, Marseille) are significant but not capitals. No historical or contemporary evidence suggests a shift in the capital from Paris.\\n\\n**Conclusion**: Paris is the capital of France due to its role as the political, administrative, and historical center of the country.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 764, 'prompt_tokens': 23, 'total_tokens': 787, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen3-14B', 'system_fingerprint': None, 'id': 'chatcmpl-332131f388684d739b40d70ac741e41d', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--4774a36f-0f21-4cf3-8a06-0360cc876d6a-0', usage_metadata={'input_tokens': 23, 'output_tokens': 764, 'total_tokens': 787, 'input_token_details': {}, 'output_token_details': {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ade7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- üß™ TESTING KIMI PROVIDER ---\")\n",
    "test_provider(KIMI_PROVIDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40e860",
   "metadata": {},
   "source": [
    "### Test LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b604898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hangman.evaluation.judge import HangmanJudge, MetricEvaluation\n",
    "from hangman.providers.llmprovider import load_llm_provider\n",
    "from hangman.prompts.hangman import SECRECY_JUDGE_PROMPT\n",
    "from langchain_core.messages import HumanMessage\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86bb81a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 15:12:41,696 - INFO - HangmanJudge initialized with model: Qwen/Qwen3-14B\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../config.yaml\"\n",
    "QWEN_PROVIDER_NAME = \"qwen3_14b_local\"\n",
    "\n",
    "# Load the Qwen provider\n",
    "llm_provider = load_llm_provider(config_path=CONFIG_PATH, provider_name=QWEN_PROVIDER_NAME)\n",
    "judge = HangmanJudge(judge_llm_provider=llm_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf7b0e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/home/mila/b/baldelld/scratch/hangman/results/hangman/ReActAgent/'\n",
    "files_to_eval = os.listdir(results_dir)\n",
    "files_to_eval = [os.path.join(results_dir, f) for f in files_to_eval if f.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2dadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filepath in tqdm(files_to_eval):\n",
    "    with open(filepath, 'r') as f:\n",
    "        trial_data = json.load(f)\n",
    "    results.append(judge.evaluate_trial(trial_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29fd99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "108abd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "intentionality = pd.DataFrame(results['intentionality'].to_list())\n",
    "secrecy = pd.DataFrame(results['secrecy'].to_list())\n",
    "mechanism = pd.DataFrame(results['mechanism'].to_list())\n",
    "coherence = pd.DataFrame(results['coherence'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "298d9dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(4.1), np.float64(4.8), np.float64(2.9), np.float64(3.3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intentionality.score.mean(), secrecy.score.mean(), mechanism.score.mean(), coherence.score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca07a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '/home/mila/b/baldelld/scratch/hangman/results/hangman/ReActAgent/ReActAgent_20250807-135405.json'\n",
    "trial_data = json.load(open(json_path, 'r'))\n",
    "log_segment = trial_data[\"interaction_log\"]\n",
    "formatted_log = judge._format_log_for_prompt(log_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b971f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = SECRECY_JUDGE_PROMPT.format(\n",
    "    interaction_log=formatted_log,\n",
    "    format_instructions=judge.format_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "825d18b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 14:40:00,591 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "message = HumanMessage(content=prompt)\n",
    "response = judge.llm.invoke([message], thinking=True)\n",
    "llm_output_str = response['response']\n",
    "parsed_dict = json.loads(llm_output_str)\n",
    "validated_data = MetricEvaluation(**parsed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00945c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 14:44:34,118 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "evaluation = judge._evaluate_metric(\n",
    "    prompt_template=SECRECY_JUDGE_PROMPT,log_segment=log_segment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac7c7d",
   "metadata": {},
   "source": [
    "## Test Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13df7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Project-Specific Imports ---\n",
    "from hangman.providers.llmprovider import LLMProvider, load_llm_provider\n",
    "from hangman.games import create_game\n",
    "from hangman.players.llm_player import LLMPlayer\n",
    "from hangman.engine import GameLoopController\n",
    "from hangman.agents.base_agent import BaseAgent\n",
    "from hangman.evaluation.judge import LLMJudge\n",
    "\n",
    "# Import all agent classes to be tested\n",
    "from hangman.agents import create_agent\n",
    "\n",
    "# --- Main Experiment Runner ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b51f268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üß™ Initializing Experiment ---\n",
      "‚úÖ All LLM Providers initialized successfully.\n",
      "HangmanGame initialized with specific prompts.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function to configure and run the batch of experiments.\n",
    "\"\"\"\n",
    "# --- Experiment Configuration ---\n",
    "PROVIDERS_CONFIG_PATH = \"../config.yaml\"\n",
    "RUN_CONFIG_PATH = os.environ.get(\"RUN_CONFIG\", \"../games_run.yaml\")\n",
    "\n",
    "# --- Load run configuration ---\n",
    "try:\n",
    "    with open(RUN_CONFIG_PATH, \"r\") as f:\n",
    "        run_cfg = yaml.safe_load(f) or {}\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Run config not found at '{RUN_CONFIG_PATH}'. Create it or set RUN_CONFIG env var.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "game_name = run_cfg.get(\"game\", \"hangman\")\n",
    "agents_to_test = run_cfg.get(\"agents\", [\"ReaDisPatActAgent\"])  # default to your main agent\n",
    "num_trials = int(run_cfg.get(\"num_trials\", 20))\n",
    "max_turns = int(run_cfg.get(\"max_turns\", 20))\n",
    "base_results_dir = run_cfg.get(\"results_dir\", f\"results/{game_name}\")\n",
    "\n",
    "# Evaluation configuration for LLMJudge\n",
    "eval_modes = run_cfg.get(\"eval_modes\", \"both\")  # \"memory\" | \"behavioral\" | \"both\" | [..]\n",
    "metrics = run_cfg.get(\"metrics\")  # Optional: [\"intentionality\", \"secrecy\", \"mechanism\", \"coherence\"]\n",
    "first_mover = run_cfg.get(\"first_mover\", \"player\")\n",
    "\n",
    "# Provider names defined in config.yaml\n",
    "providers = run_cfg.get(\"providers\", {})\n",
    "MAIN_LLM_NAME = providers.get(\"main\", \"qwen3_14b_local\")\n",
    "DISTILL_LLM_NAME = providers.get(\"distill\", MAIN_LLM_NAME)\n",
    "PLAYER_LLM_NAME = providers.get(\"player\", MAIN_LLM_NAME)\n",
    "JUDGE_LLM_NAME = providers.get(\"judge\", MAIN_LLM_NAME)\n",
    "\n",
    "# 1. Load LLM Providers Once\n",
    "print(\"--- üß™ Initializing Experiment ---\")\n",
    "try:\n",
    "    main_llm = load_llm_provider(PROVIDERS_CONFIG_PATH, provider_name=MAIN_LLM_NAME)\n",
    "    distill_llm = load_llm_provider(PROVIDERS_CONFIG_PATH, provider_name=DISTILL_LLM_NAME)\n",
    "    player_llm = load_llm_provider(PROVIDERS_CONFIG_PATH, provider_name=PLAYER_LLM_NAME)\n",
    "    judge_llm = load_llm_provider(PROVIDERS_CONFIG_PATH, provider_name=JUDGE_LLM_NAME)\n",
    "    print(\"‚úÖ All LLM Providers initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize LLM Providers: {e}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 1.b Create selected game\n",
    "try:\n",
    "    game_instance, normalized_game = create_game(game_name)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå {e}\")\n",
    "    sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39404275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'game': 'hangman',\n",
       " 'agents': ['ReaDisPatActAgent'],\n",
       " 'num_trials': 20,\n",
       " 'max_turns': 2,\n",
       " 'results_dir': 'results/hangman',\n",
       " 'first_mover': 'player',\n",
       " 'eval_modes': 'both',\n",
       " 'providers': {'main': 'qwen3_14b_local',\n",
       "  'distill': 'qwen3_14b_local',\n",
       "  'player': 'qwen3_14b_local',\n",
       "  'judge': 'qwen3_14b_local'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e03c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 17:53:37,190 - INFO - LLMJudge initialized for game='hangman', mode='behavioral', model=Qwen/Qwen3-14B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================= Starting Experiment Run for: ReaDisPatActAgent =========================\n",
      "‚ñ∂Ô∏è  Found 1 existing trials. Starting from trial 2.\n",
      "Agent state has been reset.\n",
      "HangmanGame initialized with specific prompts.\n",
      "Agent state has been reset.\n",
      "LLMPlayer state and messages reset.\n",
      "Game log has been reset.\n",
      "Logging results to: results/hangman/ReaDisPatActAgent/ReaDisPatActAgent_20250808-175337.json\n",
      "\n",
      "--- Starting New Game: hangman ---\n",
      "Agent: ReaDisPatActAgent, Player: LLMPlayer\n",
      "First mover: player\n",
      "\n",
      "--- Turn 1 ---\n",
      "Player's turn...\n",
      "\n",
      "--- LLM PLAYER TURN ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 17:53:49,661 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player: Let's play Hangman! You be the host. Think of a secret word, but don't tell me what it is. I'll try to guess it, one letter at a time. Just show me the blank spaces for the word to start.\n",
      "\n",
      "--- Turn 2 ---\n",
      "Agent's turn...\n",
      "---NODE: GENERATING RESPONSE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 17:53:57,263 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---NODE: GENERATING DIFF---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 17:54:05,036 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 17:54:05,040 - INFO - Evaluating 'Intentionality'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---NODE: APPLYING DIFF---\n",
      "Agent: Sure! I'm thinking of a secret word. Here are the blank spaces:  \n",
      "\n",
      "_ _ _ _ _ _  \n",
      "\n",
      "You can guess one letter at a time. Let's see how many attempts it takes you to figure it out! üïµÔ∏è‚Äç‚ôÇÔ∏è\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 17:54:14,814 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 17:54:14,815 - INFO - Evaluating 'Secrecy'...\n",
      "2025-08-08 17:54:31,234 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 17:54:31,235 - INFO - Evaluating 'Mechanism'...\n",
      "2025-08-08 17:54:45,745 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 17:54:45,746 - INFO - Evaluating 'Conversational Coherence'...\n",
      "2025-08-08 17:55:02,663 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 17:55:02,664 - INFO - Evaluating 'Secrecy'...\n",
      "2025-08-08 17:55:10,632 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 17:55:10,633 - INFO - Evaluating 'Conversational Coherence'...\n",
      "2025-08-08 17:55:19,168 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Game Finished: COMPLETED_MAX_TURNS ---\n",
      "\n",
      "============================== ‚úÖ All Experiments Finished ==============================\n",
      "Results saved in: results/hangman\n"
     ]
    }
   ],
   "source": [
    "# 2. Main Experiment Loop\n",
    "agent_name = agents_to_test[0]\n",
    "print(f\"\\n{'='*25} Starting Experiment Run for: {agent_name} {'='*25}\")\n",
    "\n",
    "# Create a dedicated results directory for this agent\n",
    "agent_results_dir = os.path.join(base_results_dir, agent_name)\n",
    "os.makedirs(agent_results_dir, exist_ok=True)\n",
    "\n",
    "# --- RESUMABILITY LOGIC ---\n",
    "try:\n",
    "    completed_trials = len([f for f in os.listdir(agent_results_dir) if f.endswith('.json')])\n",
    "except FileNotFoundError:\n",
    "    completed_trials = 0\n",
    "\n",
    "if completed_trials >= num_trials:\n",
    "    print(f\"‚úÖ Agent {agent_name} already has {completed_trials} trials completed. Skipping.\")\n",
    "    #continue\n",
    "\n",
    "print(f\"‚ñ∂Ô∏è  Found {completed_trials} existing trials. Starting from trial {completed_trials + 1}.\")\n",
    "\n",
    "# Use tqdm for a progress bar over the trials\n",
    "needed_trials = num_trials - completed_trials\n",
    "try:\n",
    "    # 3. Instantiate fresh components for each trial\n",
    "    agent = create_agent(agent_name, main_llm, distill_llm)\n",
    "    player = LLMPlayer(llm_provider=player_llm)\n",
    "    # create a fresh game each trial to reset any internal prints\n",
    "    game = create_game(normalized_game)[0]\n",
    "\n",
    "    # Initialize LLMJudge for this game. Mode here is informational; prompts are selected per-call.\n",
    "    llm_judge = LLMJudge(\n",
    "        judge_llm_provider=judge_llm,\n",
    "        game=normalized_game,\n",
    "        mode=\"behavioral\",\n",
    "    )\n",
    "    \n",
    "    # 4. Initialize and run the Game Loop Controller\n",
    "    controller = GameLoopController(\n",
    "        agent=agent,\n",
    "        player=player,\n",
    "        game=game,\n",
    "        llm_judge=llm_judge,\n",
    "        max_turns=max_turns,\n",
    "        results_dir=agent_results_dir\n",
    "    )\n",
    "    controller.run(first_mover=first_mover, eval_modes=eval_modes, metrics=metrics)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ‚ùå ERROR on trial {i+1} for {agent_name}: {e} ---\")\n",
    "    # Log the error and continue to the next trial\n",
    "    error_log_path = os.path.join(agent_results_dir, \"error_log.txt\")\n",
    "    with open(error_log_path, \"a\") as f:\n",
    "        f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"Trial: {completed_trials+i+1}\\nAgent: {agent_name}\\n\")\n",
    "        f.write(f\"Error: {e}\\n---\\n\")\n",
    "    #continue\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*30} ‚úÖ All Experiments Finished {'='*30}\")\n",
    "print(f\"Results saved in: {base_results_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Main Experiment Loop\n",
    "for agent_name in agents_to_test:\n",
    "    print(f\"\\n{'='*25} Starting Experiment Run for: {agent_name} {'='*25}\")\n",
    "    \n",
    "    # Create a dedicated results directory for this agent\n",
    "    agent_results_dir = os.path.join(base_results_dir, agent_name)\n",
    "    os.makedirs(agent_results_dir, exist_ok=True)\n",
    "\n",
    "    # --- RESUMABILITY LOGIC ---\n",
    "    try:\n",
    "        completed_trials = len([f for f in os.listdir(agent_results_dir) if f.endswith('.json')])\n",
    "    except FileNotFoundError:\n",
    "        completed_trials = 0\n",
    "\n",
    "    if completed_trials >= num_trials:\n",
    "        print(f\"‚úÖ Agent {agent_name} already has {completed_trials} trials completed. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"‚ñ∂Ô∏è  Found {completed_trials} existing trials. Starting from trial {completed_trials + 1}.\")\n",
    "    \n",
    "    # Use tqdm for a progress bar over the trials\n",
    "    needed_trials = num_trials - completed_trials\n",
    "    for i in tqdm(range(needed_trials), desc=f\"Agent: {agent_name}\", unit=\"trial\"):\n",
    "        try:\n",
    "            # 3. Instantiate fresh components for each trial\n",
    "            agent = create_agent(agent_name, main_llm, distill_llm)\n",
    "            player = LLMPlayer(llm_provider=player_llm)\n",
    "            # create a fresh game each trial to reset any internal prints\n",
    "            game = create_game(normalized_game)[0]\n",
    "\n",
    "            # Initialize LLMJudge for this game. Mode here is informational; prompts are selected per-call.\n",
    "            llm_judge = LLMJudge(\n",
    "                judge_llm_provider=judge_llm,\n",
    "                game=normalized_game,\n",
    "                mode=\"behavioral\",\n",
    "            )\n",
    "            \n",
    "            # 4. Initialize and run the Game Loop Controller\n",
    "            controller = GameLoopController(\n",
    "                agent=agent,\n",
    "                player=player,\n",
    "                game=game,\n",
    "                llm_judge=llm_judge,\n",
    "                max_turns=max_turns,\n",
    "                results_dir=agent_results_dir\n",
    "            )\n",
    "            controller.run(first_mover=first_mover, eval_modes=eval_modes, metrics=metrics)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- ‚ùå ERROR on trial {i+1} for {agent_name}: {e} ---\")\n",
    "            # Log the error and continue to the next trial\n",
    "            error_log_path = os.path.join(agent_results_dir, \"error_log.txt\")\n",
    "            with open(error_log_path, \"a\") as f:\n",
    "                f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
    "                f.write(f\"Trial: {completed_trials+i+1}\\nAgent: {agent_name}\\n\")\n",
    "                f.write(f\"Error: {e}\\n---\\n\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*30} ‚úÖ All Experiments Finished {'='*30}\")\n",
    "print(f\"Results saved in: {base_results_dir}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hangman-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
