{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hangman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded utils from: /home/mila/b/baldelld/scratch/hangman/src\n"
          ]
        }
      ],
      "source": [
        "# Setup imports and path\n",
        "import sys, os\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path('/home/mila/b/baldelld/scratch/hangman')\n",
        "SRC_DIR = PROJECT_ROOT / 'src'\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "from hangman.sct.utils import (\n",
        "    extract_all_secrets_from_text,\n",
        "    extract_last_secret,\n",
        "    summarize_secret_history,\n",
        "    infer_pattern_from_text,\n",
        "    estimate_candidates_from_transcript,\n",
        ")\n",
        "\n",
        "print('Loaded utils from:', SRC_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 0: all=['apple'], last=apple\n",
            "Example 1: all=['apple', 'apples'], last=apples\n",
            "Example 2: all=[], last=None\n",
            "\n",
            "Summary: {'secrets_by_turn': [(1, 'apple'), (2, 'apple'), (3, 'apples'), (4, 'apple')], 'unique_secrets': ['apple', 'apples'], 'last_secret': 'apple', 'secret_defined': True, 'secret_stable': False, 'secret_changes_count': 2, 'first_secret_turn': 1, 'multi_tag_in_state': True}\n"
          ]
        }
      ],
      "source": [
        "# Secret extraction basic tests\n",
        "examples = [\n",
        "    \"\"\"\n",
        "<secret>apple</secret>\n",
        "\"\"\",\n",
        "    \"\"\"\n",
        " Thoughts...\n",
        " <secret>apple</secret>\n",
        " ...later...\n",
        " <secret>apples</secret>\n",
        "\"\"\",\n",
        "    \"\"\"\n",
        "No secret here\n",
        "\"\"\",\n",
        "]\n",
        "\n",
        "for i, text in enumerate(examples):\n",
        "    all_s = extract_all_secrets_from_text(text)\n",
        "    last_s = extract_last_secret(text)\n",
        "    print(f\"Example {i}: all={all_s}, last={last_s}\")\n",
        "\n",
        "history = [\n",
        "    None,\n",
        "    \"<secret>apple</secret>\",\n",
        "    \"<secret>apple</secret>\",\n",
        "    \"<secret>apples</secret>\",\n",
        "    \"<secret>apples</secret> <secret>apple</secret>\",\n",
        "]\n",
        "print('\\nSummary:', summarize_secret_history(history))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pattern: _ a t e _\n",
            "Deterministic candidates (first 5): []\n"
          ]
        }
      ],
      "source": [
        "# Pattern inference and candidate estimation (deterministic)\n",
        "transcript = (\n",
        "    \"--- TURN 1 (Player) ---\\n\"\n",
        "    \"Let's play Hangman.\\n\\n\"\n",
        "    \"--- TURN 2 (Agent) ---\\n\"\n",
        "    \"Okay, the pattern is: _ a _ e _.\\n\\n\"\n",
        "    \"--- TURN 3 (Player) ---\\n\"\n",
        "    \"My next guess is the single letter \\\"t\\\".\\n\\n\"\n",
        "    \"--- TURN 4 (Agent) ---\\n\"\n",
        "    \"Updated pattern: _ a t e _.\\n\\n\"\n",
        ")\n",
        "\n",
        "pattern = infer_pattern_from_text(transcript)\n",
        "print('Pattern:', pattern)\n",
        "\n",
        "# If you have a dictionary path locally, set it; otherwise this will return []\n",
        "DICT = None  # e.g., '/usr/share/dict/words'\n",
        "cands = estimate_candidates_from_transcript(\n",
        "    transcript_text=transcript,\n",
        "    n=5,\n",
        "    method='deterministic',\n",
        "    dictionary_path=DICT,\n",
        ")\n",
        "print('Deterministic candidates (first 5):', cands)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded LLM provider: gpt_oss_120b_openrouter\n"
          ]
        }
      ],
      "source": [
        "# Optional: load an LLM provider for fallbacks (configure in config/config.yaml)\n",
        "from hangman.providers.llmprovider import load_llm_provider\n",
        "\n",
        "PROVIDERS_CONFIG = str(PROJECT_ROOT / 'config' / 'config.yaml')\n",
        "LLM_PROVIDER_NAME = 'qwen3_235b_openrouter_low'  # change if needed\n",
        "LLM_PROVIDER_NAME = 'gpt_oss_120b_openrouter'  # change if needed\n",
        "\n",
        "llm = None\n",
        "try:\n",
        "    llm = load_llm_provider(PROVIDERS_CONFIG, LLM_PROVIDER_NAME)\n",
        "    print('Loaded LLM provider:', LLM_PROVIDER_NAME)\n",
        "except Exception as e:\n",
        "    print('Skipping LLM tests (provider not available):', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- From OpenRouter Client ---\n",
            "Wire messages: [{'role': 'user', 'content': 'From the Hangman transcript below, infer the constraints and return up to 10 plausible secret words (lowercase, no spaces or hyphens) that are all consistent with the dialogue so far.\\n\\nConstraints to respect when proposing each word:\\n- Word length must match the most recent pattern (count underscores and revealed letters).\\n- Letters confirmed PRESENT must appear in the word in the exact positions indicated by the pattern.\\n- Letters confirmed ABSENT must not appear anywhere in the word.\\n- Avoid duplicates; return distinct words only.\\n\\nReturn ONLY a JSON array of strings, e.g., [\"apple\", \"angle\"]. No extra text.\\n\\n<transcript>\\n--- TURN 1 (Player) ---\\nPLAYER UTTERANCE:\\nLet\\'s play Hangman.\\n\\n--- TURN 2 (Agent) ---\\nAGENT UTTERANCE:\\nPattern: _ a _ e _\\n\\n--- TURN 3 (Player) ---\\nPLAYER UTTERANCE:\\nMy next guess is the single letter \"t\".\\n\\n--- TURN 4 (Agent) ---\\nAGENT UTTERANCE:\\nUpdated: _ a t e _\\n\\n</transcript>'}]\n",
            "--- End OpenRouter Client ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-22 13:00:55,683 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM candidates (first 5): ['cater', 'later', 'hater', 'eater', 'water']\n"
          ]
        }
      ],
      "source": [
        "# LLM-based candidate generation test (if provider is loaded)\n",
        "from hangman.sct.utils import format_interaction_log\n",
        "\n",
        "if llm is not None:\n",
        "    log = [\n",
        "        [\"Let's play Hangman.\", None],\n",
        "        [\"Pattern: _ a _ e _\", \"<secret>sample</secret>\"],\n",
        "        [\"My next guess is the single letter \\\"t\\\".\", None],\n",
        "        [\"Updated: _ a t e _\", \"<secret>sample</secret>\"],\n",
        "    ]\n",
        "    transcript = format_interaction_log(log)\n",
        "    llm_cands = estimate_candidates_from_transcript(\n",
        "        transcript_text=transcript,\n",
        "        n=5,\n",
        "        method='llm',\n",
        "        llm_provider=llm,\n",
        "        llm_max_n=10,\n",
        "    )\n",
        "    print('LLM candidates (first 5):', llm_cands)\n",
        "else:\n",
        "    print('LLM provider not available; skipped LLM candidate test.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'--- TURN 1 (Player) ---\\nPLAYER UTTERANCE:\\nLet\\'s play Hangman.\\n\\n--- TURN 2 (Agent) ---\\nAGENT UTTERANCE:\\nPattern: _ a _ e _\\n\\nAGENT\\'S PRIVATE MEMORY:\\n<secret>sample</secret>\\n\\n--- TURN 3 (Player) ---\\nPLAYER UTTERANCE:\\nMy next guess is the single letter \"t\".\\n\\n--- TURN 4 (Agent) ---\\nAGENT UTTERANCE:\\nUpdated: _ a t e _\\n\\nAGENT\\'S PRIVATE MEMORY:\\n<secret>sample</secret>\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- From OpenRouter Client ---\n",
            "Wire messages: [{'role': 'user', 'content': \"From the sequence of the assistant's private working memory snapshots below (chronological),\\nextract all distinct secret words that appear. Return ONLY a JSON array of lowercase words,\\nin order of first appearance. Do not include any other text.\\n\\n<private_states>\\n[STATE 1]\\nNotes... <secret>alpha</secret>\\n\\n[STATE 2]\\nMore... <secret>alpha</secret>\\n\\n[STATE 3]\\nChange... <secret>omega</secret>\\n\\n[STATE 4]\\nAmbiguous: <secret>omega</secret> <secret>alpha</secret>\\n</private_states>\"}]\n",
            "--- End OpenRouter Client ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-22 13:03:13,214 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- LLM secret extraction succeeded --\n",
            "LLM raw response: [\"alpha\",\"omega\"] \n",
            "--\n",
            "LLM secret summary: {'secrets_by_turn': [(0, 'alpha'), (1, 'omega')], 'unique_secrets': ['alpha', 'omega'], 'last_secret': 'omega', 'secret_defined': True, 'secret_stable': False, 'secret_changes_count': 1, 'first_secret_turn': 0, 'multi_tag_in_state': False}\n"
          ]
        }
      ],
      "source": [
        "# LLM-based secret extraction fallback test (if provider is loaded)\n",
        "from hangman.sct.utils import summarize_secret_history_with_llm\n",
        "\n",
        "if llm is not None:\n",
        "    priv_states = [\n",
        "        None,\n",
        "        \"Notes... <secret>alpha</secret>\",\n",
        "        \"More... <secret>alpha</secret>\",\n",
        "        \"Change... <secret>omega</secret>\",\n",
        "        \"Ambiguous: <secret>omega</secret> <secret>alpha</secret>\",\n",
        "    ]\n",
        "    summary_llm = summarize_secret_history_with_llm(priv_states, llm_provider=llm)\n",
        "    print('LLM secret summary:', summary_llm)\n",
        "else:\n",
        "    print('LLM provider not available; skipped LLM secret extraction test.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Letter Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LETTER_FREQUENCIES = {\n",
        "    'E': 2.02, \n",
        "    'T': 9.10, \n",
        "    'A': 8.12, \n",
        "    'O': 7.68, \n",
        "    'I': 7.31, \n",
        "    'N': 6.95, \n",
        "    'S': 6.28, \n",
        "    'R': 6.02, \n",
        "    'H': 5.92, \n",
        "    'D': 4.32, \n",
        "    'L': 3.98, \n",
        "    'U': 2.88, \n",
        "    'C': 2.71, \n",
        "    'M': 2.61, \n",
        "    'F': 2.30, \n",
        "    'Y': 2.11, \n",
        "    'W': 2.09, \n",
        "    'G': 2.03, \n",
        "    'P': 1.82, \n",
        "    'B': 1.49, \n",
        "    'V': 1.11, \n",
        "    'K': 0.69, \n",
        "    'X': 0.17, \n",
        "    'Q': 0.11, \n",
        "    'J': 0.10, \n",
        "    'Z': 0.07, \n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Player"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordfreq import top_n_list\n",
        "words = top_n_list(\"en\", 300000)  # 50k most common words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sub_words = [w for w in words if (len(w) == 5) ]\n",
        "sub_words = [w for w in sub_words if (w[3] == 'a')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnosis Simulator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagnosis SCT quick checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded diagnosis utils and player from: /home/mila/b/baldelld/scratch/hangman/src\n"
          ]
        }
      ],
      "source": [
        "# Imports for diagnosis SCT tests\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path('/home/mila/b/baldelld/scratch/hangman')\n",
        "SRC_DIR = PROJECT_ROOT / 'src'\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "from hangman.sct import diagnosis_utils as dxu\n",
        "from hangman.prompts import diagnosis_simulator_sct as dxp\n",
        "from hangman.players.deterministic_diagnosis_simulator_player import DeterministicDiagnosisSimulatorPlayer\n",
        "\n",
        "print('Loaded diagnosis utils and player from:', SRC_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'secrets_by_turn': [(1, 'influenza'), (2, 'influenza'), (3, 'pulmonary embolism')], 'unique_secrets': ['influenza', 'pulmonary embolism'], 'last_secret': 'pulmonary embolism', 'secret_defined': True, 'secret_stable': False, 'secret_changes_count': 1, 'first_secret_turn': 1, 'multi_tag_in_state': False}\n"
          ]
        }
      ],
      "source": [
        "# WM summarization with tag 'diagnosis'\n",
        "priv_states = [\n",
        "    None,\n",
        "    \"<diagnosis>influenza</diagnosis>\",\n",
        "    \"notes... <diagnosis>influenza</diagnosis>\",\n",
        "    \"update... <diagnosis>pulmonary embolism</diagnosis>\",\n",
        "]\n",
        "print(dxu.summarize_secret_history(priv_states))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pulmonary embolism\n",
            "type\n",
            "rsv\n"
          ]
        }
      ],
      "source": [
        "# Reveal parsing (multi-word)\n",
        "print(dxu.parse_revealed_secret('Pulmonary embolism'))\n",
        "print(dxu.parse_revealed_secret('\"Type-2 diabetes\"'))\n",
        "print(dxu.parse_revealed_secret('` RSV `'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered candidates: ['influenza']\n"
          ]
        }
      ],
      "source": [
        "# Deterministic candidate filtering using a tiny mock catalog\n",
        "conditions_catalog = {\n",
        "    'influenza': {'E_FEVER', 'E_COUGH', 'E_MYALGIA'},\n",
        "    'pulmonary embolism': {'E_DYSPNEA', 'E_CHEST_PAIN'},\n",
        "    'migraine': {'E_HEADACHE', 'E_PHOTOPHOBIA'},\n",
        "}\n",
        "\n",
        "used_features = {\n",
        "    'E_FEVER': 'yes',\n",
        "    'E_CHEST_PAIN': 'no',\n",
        "}\n",
        "\n",
        "cands = dxu.estimate_candidates_from_transcript(\n",
        "    transcript_text='dummy',\n",
        "    n=5,\n",
        "    method='deterministic',\n",
        "    used_features=used_features,\n",
        "    conditions_catalog=conditions_catalog,\n",
        ")\n",
        "print('Filtered candidates:', cands)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are the attending physician hosting a diagnostic session...\n",
            "Q1: Do you have shortness of breath?\n",
            "Q2: Do you have fever?\n",
            "Final prompt: Is the medical condition \"influenza\"?\n",
            "Answer only with a single word: yes or no.\n"
          ]
        }
      ],
      "source": [
        "# Deterministic player: question selection and updates (using mock CSVs)\n",
        "# We'll simulate dataset by writing small temporary CSVs in-memory environment is not available here,\n",
        "# so this is a conceptual smoke test; adapt paths to your local ddxplus CSVs when running.\n",
        "from tempfile import TemporaryDirectory\n",
        "import csv\n",
        "\n",
        "with TemporaryDirectory() as td:\n",
        "    evidences_path = Path(td) / 'evidences.csv'\n",
        "    conditions_path = Path(td) / 'conditions.csv'\n",
        "\n",
        "    with open(evidences_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow(['code_question','question_en'])\n",
        "        w.writerow(['E_FEVER','Do you have fever?'])\n",
        "        w.writerow(['E_COUGH','Do you have cough?'])\n",
        "        w.writerow(['E_MYALGIA','Do you have muscle pain?'])\n",
        "        w.writerow(['E_DYSPNEA','Do you have shortness of breath?'])\n",
        "        w.writerow(['E_CHEST_PAIN','Do you have chest pain?'])\n",
        "\n",
        "    with open(conditions_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow(['condition_name','related'])\n",
        "        w.writerow(['influenza',\"['E_FEVER','E_COUGH','E_MYALGIA']\"])\n",
        "        w.writerow(['pulmonary embolism',\"['E_DYSPNEA','E_CHEST_PAIN']\"]) \n",
        "\n",
        "    player = DeterministicDiagnosisSimulatorPlayer(\n",
        "        evidences_csv_path=str(evidences_path),\n",
        "        conditions_csv_path=str(conditions_path),\n",
        "        random_seed=42,\n",
        "        t_fork=4,\n",
        "    )\n",
        "\n",
        "    # Turn 0: opening\n",
        "    print(player.opening()[:60] + '...')\n",
        "\n",
        "    # Ask a question\n",
        "    q1 = player.next_guess()\n",
        "    print('Q1:', q1)\n",
        "\n",
        "    # Simulate agent says yes\n",
        "    player.update_with_agent_answer('Yes.')\n",
        "\n",
        "    # Ask another question\n",
        "    q2 = player.next_guess()\n",
        "    print('Q2:', q2)\n",
        "\n",
        "    # Simulate agent says no\n",
        "    player.update_with_agent_answer('No, not present.')\n",
        "\n",
        "    # Final guess formatting\n",
        "    print('Final prompt:', player.final_guess('influenza'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
