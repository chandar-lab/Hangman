{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d009292",
   "metadata": {},
   "source": [
    "# A-mem: Agentic Memory with OpenRouter\n",
    "\n",
    "This notebook demonstrates how to use the A-mem (Agentic Memory) system with OpenRouter.\n",
    "\n",
    "A-mem provides:\n",
    "- **Automatic metadata generation** using LLMs (keywords, context, tags)\n",
    "- **Semantic search** using ChromaDB vector embeddings\n",
    "- **Memory evolution** with automatic relationship discovery\n",
    "- **Multiple LLM backends** including OpenAI, Ollama, and OpenRouter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc043e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:23:13,299 - ERROR - Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-10-16 14:23:13,299 - ERROR - Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-10-16 14:23:13,322 - ERROR - Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-10-16 14:23:13,322 - ERROR - Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… A-mem initialized with OpenRouter backend!\n"
     ]
    }
   ],
   "source": [
    "# Setup and initialization\n",
    "from agentic_memory.memory_system import AgenticMemorySystem\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize A-mem with OpenRouter backend\n",
    "memory_system = AgenticMemorySystem(\n",
    "    model_name='all-MiniLM-L6-v2',           # Embedding model for ChromaDB\n",
    "    llm_backend=\"openrouter\",                 # Use OpenRouter backend\n",
    "    llm_model=\"openai/gpt-oss-20b\",  # Free model from OpenRouter\n",
    "    api_key=os.getenv('OPENROUTER_API_KEY')\n",
    ")\n",
    "\n",
    "print(\"âœ… A-mem initialized with OpenRouter backend!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "070b1344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:16 - LiteLLM:INFO\u001b[0m: utils.py:3373 - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n",
      "2025-10-16 14:23:16,203 - INFO - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n",
      "/home/mila/b/baldelld/scratch/hangman/.venv/lib/python3.11/site-packages/httpx/_models.py:408: DeprecationWarning: Use 'content=<...>' to upload raw bytes/text content.\n",
      "  headers, stream = encode_request(\n",
      "\u001b[92m14:23:21 - LiteLLM:INFO\u001b[0m: utils.py:1286 - Wrapper: Completed Call, calling success_handler\n",
      "2025-10-16 14:23:21,985 - INFO - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing content: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a1dbc5989243b4b758d1fdb6c49d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:23:22,036 - ERROR - Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
      "\u001b[92m14:23:22 - LiteLLM:INFO\u001b[0m: utils.py:3373 - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n",
      "2025-10-16 14:23:22,045 - INFO - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Added memory: 7e4e09be...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:29 - LiteLLM:INFO\u001b[0m: utils.py:1286 - Wrapper: Completed Call, calling success_handler\n",
      "2025-10-16 14:23:29,255 - INFO - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707ea7c5d28048c7be0000dd9b3d6d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:23:29,293 - ERROR - Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "\u001b[92m14:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3373 - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n",
      "2025-10-16 14:23:29,298 - INFO - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n",
      "\u001b[92m14:24:23 - LiteLLM:INFO\u001b[0m: utils.py:1286 - Wrapper: Completed Call, calling success_handler\n",
      "2025-10-16 14:24:23,628 - INFO - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c041922c2fc2453fbb9e223d02ccf0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:24:23 - LiteLLM:INFO\u001b[0m: utils.py:3373 - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n",
      "2025-10-16 14:24:23,685 - INFO - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Added memory: 975cbcc8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: utils.py:1286 - Wrapper: Completed Call, calling success_handler\n",
      "2025-10-16 14:24:47,244 - INFO - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdab2bba542a466bba26d4ae18d8766d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: utils.py:3373 - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n",
      "2025-10-16 14:24:47,281 - INFO - \n",
      "LiteLLM completion() model= openai/gpt-oss-20b; provider = openrouter\n",
      "\u001b[92m14:24:54 - LiteLLM:INFO\u001b[0m: utils.py:1286 - Wrapper: Completed Call, calling success_handler\n",
      "2025-10-16 14:24:54,868 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-10-16 14:24:54,870 - ERROR - Error in memory evolution: Expecting property name enclosed in double quotes: line 2 column 1 (char 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30f2fc206f045f99b8ad7ecae43fa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Added memory: 35f41abf...\n",
      "\n",
      "âœ… Added 3 memories with automatic metadata generation!\n",
      "\n",
      "ðŸ“ Example memory:\n",
      "   Content: Machine learning models use neural networks to process complex datasets and identify patterns.\n",
      "   Keywords: []\n",
      "   Tags: []\n",
      "   Context: General\n"
     ]
    }
   ],
   "source": [
    "# Add memories - LLM automatically generates keywords, context, and tags\n",
    "memories = [\n",
    "    \"Machine learning models use neural networks to process complex datasets and identify patterns.\",\n",
    "    \"Python is excellent for data science with libraries like pandas, numpy, and scikit-learn.\",\n",
    "    \"The Hangman game requires agents to maintain hidden state while responding to player guesses.\"\n",
    "]\n",
    "\n",
    "memory_ids = []\n",
    "for content in memories:\n",
    "    memory_id = memory_system.add_note(content)\n",
    "    memory_ids.append(memory_id)\n",
    "    print(f\"âœ“ Added memory: {memory_id[:8]}...\")\n",
    "\n",
    "print(f\"\\nâœ… Added {len(memory_ids)} memories with automatic metadata generation!\")\n",
    "\n",
    "# Inspect the first memory to see auto-generated metadata\n",
    "first_memory = memory_system.read(memory_ids[0])\n",
    "print(f\"\\nðŸ“ Example memory:\")\n",
    "print(f\"   Content: {first_memory.content}\")\n",
    "print(f\"   Keywords: {first_memory.keywords}\")\n",
    "print(f\"   Tags: {first_memory.tags}\")\n",
    "print(f\"   Context: {first_memory.context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c36bba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memories': {'7e4e09be-3fd5-460c-9caf-84a3c6667c13': <agentic_memory.memory_system.MemoryNote at 0x7fb1334c1c90>,\n",
       "  '975cbcc8-74cc-40a4-bc58-420c7bbb6169': <agentic_memory.memory_system.MemoryNote at 0x7fb120620350>,\n",
       "  '35f41abf-7887-47de-b819-78fec4438c57': <agentic_memory.memory_system.MemoryNote at 0x7fb10fda6890>},\n",
       " 'model_name': 'all-MiniLM-L6-v2',\n",
       " 'retriever': <agentic_memory.retrievers.ChromaRetriever at 0x7fb12087d110>,\n",
       " 'llm_controller': <agentic_memory.llm_controller.LLMController at 0x7fb133327010>,\n",
       " 'evo_cnt': 0,\n",
       " 'evo_threshold': 100,\n",
       " '_evolution_system_prompt': '\\n                                You are an AI memory evolution agent responsible for managing and evolving a knowledge base.\\n                                Analyze the the new memory note according to keywords and context, also with their several nearest neighbors memory.\\n                                Make decisions about its evolution.  \\n\\n                                The new memory context:\\n                                {context}\\n                                content: {content}\\n                                keywords: {keywords}\\n\\n                                The nearest neighbors memories:\\n                                {nearest_neighbors_memories}\\n\\n                                Based on this information, determine:\\n                                1. Should this memory be evolved? Consider its relationships with other memories.\\n                                2. What specific actions should be taken (strengthen, update_neighbor)?\\n                                   2.1 If choose to strengthen the connection, which memory should it be connected to? Can you give the updated tags of this memory?\\n                                   2.2 If choose to update_neighbor, you can update the context and tags of these memories based on the understanding of these memories. If the context and the tags are not updated, the new context and tags should be the same as the original ones. Generate the new context and tags in the sequential order of the input neighbors.\\n                                Tags should be determined by the content of these characteristic of these memories, which can be used to retrieve them later and categorize them.\\n                                Note that the length of new_tags_neighborhood must equal the number of input neighbors, and the length of new_context_neighborhood must equal the number of input neighbors.\\n                                The number of neighbors is {neighbor_number}.\\n                                Return your decision in JSON format with the following structure:\\n                                {{\\n                                    \"should_evolve\": True or False,\\n                                    \"actions\": [\"strengthen\", \"update_neighbor\"],\\n                                    \"suggested_connections\": [\"neighbor_memory_ids\"],\\n                                    \"tags_to_update\": [\"tag_1\",...\"tag_n\"], \\n                                    \"new_context_neighborhood\": [\"new context\",...,\"new context\"],\\n                                    \"new_tags_neighborhood\": [[\"tag_1\",...,\"tag_n\"],...[\"tag_1\",...,\"tag_n\"]],\\n                                }}\\n                                '}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_system.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b630c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Machine learning models use neural networks to process complex datasets and identify patterns.',\n",
       " 'id': 'df86b285-17a0-43b2-a335-c88ab3a5f629',\n",
       " 'keywords': ['neural networks',\n",
       "  'machine learning models',\n",
       "  'complex datasets',\n",
       "  'identify patterns'],\n",
       " 'links': [],\n",
       " 'context': 'General',\n",
       " 'category': 'Uncategorized',\n",
       " 'tags': ['Machine Learning', 'Neural Networks', 'Data Science'],\n",
       " 'timestamp': '202510161156',\n",
       " 'last_accessed': '202510161156',\n",
       " 'retrieval_count': 0,\n",
       " 'evolution_history': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_memory.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d359baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'artificial intelligence and data analysis'\n",
      "\n",
      "1. Memory ID: df86b285...\n",
      "   Content: Machine learning models use neural networks to process complex datasets and iden...\n",
      "   Keywords: ['neural networks', 'machine learning models', 'complex datasets', 'identify patterns']\n",
      "\n",
      "2. Memory ID: df8127b2...\n",
      "   Content: Python is excellent for data science with libraries like pandas, numpy, and scik...\n",
      "   Keywords: ['Python', 'data science', 'libraries', 'pandas', 'numpy', 'scikit-learn']\n",
      "\n",
      "3. Memory ID: 38d5a162...\n",
      "   Content: The Hangman game requires agents to maintain hidden state while responding to pl...\n",
      "   Keywords: []\n",
      "\n",
      "âœ… Semantic search completed! The system uses both content and metadata for retrieval.\n"
     ]
    }
   ],
   "source": [
    "# Search memories using semantic similarity\n",
    "query = \"artificial intelligence and data analysis\"\n",
    "\n",
    "print(f\"ðŸ” Searching for: '{query}'\\n\")\n",
    "results = memory_system.search(query, k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Memory ID: {result['id'][:8]}...\")\n",
    "    print(f\"   Content: {result['content'][:80]}...\")\n",
    "    print(f\"   Keywords: {result['keywords']}\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Semantic search completed! The system uses both content and metadata for retrieval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5993b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/agiresearch/A-mem.git\n",
    "import os\n",
    "import yaml\n",
    "from collections import deque\n",
    "from typing import List, Any, Dict, Optional\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from hangman.agents.base_agent import BaseAgent, ModelOutput\n",
    "from hangman.providers.llmprovider import LLMProvider, load_llm_provider\n",
    "\n",
    "# A-mem system\n",
    "from agentic_memory.memory_system import AgenticMemorySystem  # from the repo\n",
    "\n",
    "# ---- System prompt for A-mem ----\n",
    "AMEM_MAIN_SYSTEM_PROMPT = \"\"\"You are a helpful assistant that uses retrieved agentic memory notes to maintain consistency, continuity, and factual accuracy across turns.\n",
    "\n",
    "# HOW TO USE THE NOTES\n",
    "- Treat each note as an atomic, trustworthy recollection from prior interactions.\n",
    "- Use the *content* directly; use *keywords* and *tags* to infer topical relevance; use *context* to disambiguate or enrich meaning.\n",
    "- Weave relevant information naturally into your reply; do not list the notes verbatim unless explicitly requested.\n",
    "- If none are relevant, proceed normally while staying consistent with any known information.\n",
    "\n",
    "# RETRIEVED NOTES (most relevant first)\n",
    "{notes_block}\n",
    "\"\"\"\n",
    "\n",
    "class AgentState(TypedDict, total=False):\n",
    "    messages: List[BaseMessage]\n",
    "    thinking: str\n",
    "    last_query: str\n",
    "    last_system_prompt: str\n",
    "    retrieved_notes: List[Dict[str, Any]]\n",
    "\n",
    "class AMemAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Paper-faithful A-mem baseline:\n",
    "      - On each turn: add a new note from the latest HumanMessage (A-mem auto-generates K/G/X, embeds, links, evolves).\n",
    "      - Retrieval: search top-k notes for the latest user query.\n",
    "      - Generation: System(MEMORY NOTES) + last m Human/AI messages.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_provider: LLMProvider,\n",
    "        *,\n",
    "        amem_model_name: str = \"all-MiniLM-L6-v2\",  # Embedding model\n",
    "        amem_llm_backend: str = \"openrouter\",           # or \"ollama\"\n",
    "        amem_llm_model: str = \"openai/gpt-oss-20b\",\n",
    "        session_ns: str = \"amem_session_1\",\n",
    "        m_recent: int = 10,\n",
    "        k_retrieve: int = 8,\n",
    "    ):\n",
    "        self.session_ns = session_ns\n",
    "        self.m_recent = int(m_recent)\n",
    "        self.k_retrieve = int(k_retrieve)\n",
    "\n",
    "        # Initialize A-mem memory system\n",
    "        # If using OpenRouter via OpenAI SDK:\n",
    "        #   export OPENAI_BASE_URL=https://openrouter.ai/api/v1\n",
    "        #   export OPENAI_API_KEY=$OPENROUTER_API_KEY\n",
    "        self.amem = AgenticMemorySystem(\n",
    "            model_name=amem_model_name,\n",
    "            llm_backend=amem_llm_backend,\n",
    "            llm_model=amem_llm_model,\n",
    "        )\n",
    "\n",
    "        # Sliding window for composing messages to the LLM (not for A-mem ingestion)\n",
    "        self._window: deque = deque([], maxlen=self.m_recent)\n",
    "\n",
    "        super().__init__(llm_provider=llm_provider)\n",
    "        self.reset()\n",
    "\n",
    "    def _build_workflow(self) -> StateGraph:\n",
    "        wf = StateGraph(AgentState)\n",
    "        wf.add_node(\"agent\", self._agent_node)\n",
    "        wf.set_entry_point(\"agent\")\n",
    "        return wf.compile(checkpointer=MemorySaver())\n",
    "\n",
    "    # ---------- Core node ----------\n",
    "    def _agent_node(self, state: AgentState) -> Dict[str, Any]:\n",
    "        incoming: List[BaseMessage] = state.get(\"messages\", [])\n",
    "\n",
    "        # Update conversation window (for model history composition)\n",
    "        for msg in incoming:\n",
    "            if isinstance(msg, (HumanMessage, AIMessage)):\n",
    "                self._window.append(msg)\n",
    "\n",
    "        # Latest user text is the \"interaction content\" to turn into a note\n",
    "        latest_user = \"\"\n",
    "        for msg in reversed(incoming):\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                latest_user = str(msg.content)\n",
    "                break\n",
    "\n",
    "        # 1) NOTE CONSTRUCTION (+ auto K/G/X/embedding), LINK GEN, EVOLUTION\n",
    "        if latest_user:\n",
    "            # A-mem auto-adds: generates keywords/tags/context, embeds, links, evolves\n",
    "            try:\n",
    "                _note_id = self.amem.add_note(content=latest_user)\n",
    "            except Exception:\n",
    "                pass  # don't crash the turn if A-mem fails\n",
    "\n",
    "        # 2) RETRIEVAL for generation\n",
    "        retrieved = []\n",
    "        try:\n",
    "            results = self.amem.search(latest_user or \"\", k=self.k_retrieve) or []\n",
    "            # normalize shape into a list of dicts we can print\n",
    "            for r in results:\n",
    "                # Common fields from README: id, content, keywords, tags, (maybe score/context)\n",
    "                retrieved.append({\n",
    "                    \"id\": r.get(\"id\"),\n",
    "                    \"content\": r.get(\"content\") or r.get(\"note\") or \"\",\n",
    "                    \"keywords\": r.get(\"keywords\") or [],\n",
    "                    \"tags\": r.get(\"tags\") or [],\n",
    "                    \"context\": r.get(\"context\") or r.get(\"description\") or \"\",\n",
    "                    \"score\": r.get(\"score\"),\n",
    "                })\n",
    "        except Exception:\n",
    "            retrieved = []\n",
    "\n",
    "        # Build a compact notes block for the system prompt\n",
    "        if retrieved:\n",
    "            lines = []\n",
    "            for n in retrieved:\n",
    "                kw = \", \".join(n[\"keywords\"]) if n[\"keywords\"] else \"-\"\n",
    "                tg = \", \".join(n[\"tags\"]) if n[\"tags\"] else \"-\"\n",
    "                cx = n[\"context\"][:300] + (\"...\" if n[\"context\"] and len(n[\"context\"]) > 300 else \"\")\n",
    "                ct = n[\"content\"][:400] + (\"...\" if n[\"content\"] and len(n[\"content\"]) > 400 else \"\")\n",
    "                lines.append(f\"- Content: {ct}\\n  Keywords: {kw}\\n  Tags: {tg}\\n  Context: {cx}\")\n",
    "            notes_block = \"\\n\".join(lines)\n",
    "        else:\n",
    "            notes_block = \"(none)\"\n",
    "\n",
    "        system_prompt = AMEM_MAIN_SYSTEM_PROMPT.format(notes_block=notes_block)\n",
    "\n",
    "        # 3) Compose messages like your README: System(memory) + last m turns\n",
    "        messages_for_model: List[BaseMessage] = [SystemMessage(content=system_prompt)] + list(self._window)\n",
    "\n",
    "        # 4) Call your LLM provider\n",
    "        response_obj = self.llm_provider.client.invoke(messages_for_model)\n",
    "        try:\n",
    "            parsed = self.llm_provider.parse_response(response_obj.content or \"\")\n",
    "            final_text = parsed.get(\"response\") or (response_obj.content or \"\")\n",
    "            thinking = parsed.get(\"thinking\", \"\")\n",
    "        except Exception:\n",
    "            final_text = response_obj.content or \"\"\n",
    "            thinking = \"\"\n",
    "\n",
    "        ai_msg = AIMessage(content=final_text)\n",
    "\n",
    "        return {\n",
    "            \"messages\": state.get(\"messages\", []) + [ai_msg],\n",
    "            \"thinking\": thinking,\n",
    "            \"last_query\": latest_user,\n",
    "            \"last_system_prompt\": system_prompt,\n",
    "            \"retrieved_notes\": retrieved,\n",
    "        }\n",
    "\n",
    "    # ---------- BaseAgent interface ----------\n",
    "    def invoke(self, messages: List[BaseMessage]) -> ModelOutput:\n",
    "        cfg = {\"configurable\": {\"thread_id\": f\"amem::{self.session_ns}\"}}\n",
    "        final_state = self.workflow.invoke({\"messages\": messages}, config=cfg)\n",
    "        final_response = \"\"\n",
    "        for m in reversed(final_state[\"messages\"]):\n",
    "            if isinstance(m, AIMessage):\n",
    "                final_response = m.content\n",
    "                break\n",
    "        return {\"response\": final_response, \"thinking\": final_state.get(\"thinking\", \"\")}\n",
    "\n",
    "    def get_state(self) -> Dict[str, Any]:\n",
    "        snap = self.workflow.get_state({\"configurable\": {\"thread_id\": f\"amem::{self.session_ns}\"}})\n",
    "        return snap.values if snap else {}\n",
    "\n",
    "    def get_private_state(self) -> str:\n",
    "        # summarize retrieved last turn for logging\n",
    "        st = self.get_state()\n",
    "        notes = st.get(\"retrieved_notes\", []) or []\n",
    "        if not notes:\n",
    "            return \"(no retrieved notes)\"\n",
    "        lines = []\n",
    "        for n in notes:\n",
    "            lines.append(f\"- {n.get('content','')[:120]}  | kw=[{', '.join(n.get('keywords',[]))}]  tags=[{', '.join(n.get('tags',[]))}]\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._window.clear()\n",
    "        empty: AgentState = AgentState(messages=[], thinking=\"\")\n",
    "        self.workflow.update_state({\"configurable\": {\"thread_id\": f\"amem::{self.session_ns}\"}}, empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb0cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
