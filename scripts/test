#!/bin/bash
#SBATCH --job-name=smoke_test_hangman
#SBATCH --output=logs/output/smoke-%A.%a.out
#SBATCH --error=logs/error/smoke-%A.%a.err
#SBATCH --time=0-00:20:00
#SBATCH --nodes=1
#SBATCH --gpus-per-task=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=0
#SBATCH --ntasks-per-node=1
#SBATCH --partition=short-unkillable
#SBATCH --constraint=80gb               # ensure A100 80GB

# Cd to project root
cd /home/mila/b/baldelld/scratch/hangman

# Create directories for logs
mkdir -p logs/output logs/error

# Activate the virtual environment
source .venv/bin/activate || true

# --- 1. Start single-GPU vLLM Server in the Background ---
echo "--- Starting vLLM Server (single GPU) ---"
MODEL=${MODEL:-Qwen/Qwen3-14B} GPU_ID=${GPU_ID:-0} PORT=${PORT:-8001} DTYPE=${DTYPE:-bfloat16} \
  ./run_qwen_vllm_native_server_singlegpu.sh &

# Get the Process ID (PID) of the vLLM server
VLLM_PID=$!
echo "vLLM Server started with PID: $VLLM_PID"

# --- 2. Wait for the Server to be Ready ---
echo "--- Waiting for vLLM server to be ready on port ${PORT:-8001} ---"
until curl -s http://localhost:${PORT:-8001}/health > /dev/null; do
  echo "Server not yet available. Waiting 5 seconds..."
  sleep 5
done
echo "âœ… vLLM Server is ready."

# --- 3. Run the Experiment (smoke) ---
echo "--- Starting Smoke Test Run ---"
python run_experiment.py --run-config ./config/test.yaml

# --- 4. Clean Up ---
echo "--- Smoke test finished. Shutting down vLLM server. ---"
kill $VLLM_PID || true
wait $VLLM_PID 2>/dev/null || true



